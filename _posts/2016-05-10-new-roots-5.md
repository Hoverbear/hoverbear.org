---
layout: post
title: "New Roots part 5, Container Infrastructure"
author: "Andrew Hobden"
image: /assets/images/2016/05/new-roots-5.jpg
image-credit: "Ethan Dow"
tags:
- New Roots
- Servers
- Infrastructure
- Arch Linux
---

This is the fifth part of an ongoing series on configuring a new server. In our [last post](/2016/05/09/new-roots-4/) we discussed and configured some basic tools. For all intensive purposes, our 'root' system is complete. What we'll be doing now is building the infrastructure to run containers the way we want to.

Before we get around to setting things up, let's describe the what we're up to.

Currently, the only external service the server is running is `ssh`. We'd like our server to host more services. These services are likely to be both internal services like databases, and external services like HTTP/HTTPS hosts. Since the IPv4 space is increasingly crowded we'd like to do all of these things from one IPv4 address too.

There are lots of ways to provide isolation to these services. These include virtual machines through something like [Xen](http://xenproject.org/), containers through something like [Docker](https://www.docker.com/), or even just chroots. Isolation isn't just for security, it's one way to help increase your security *somewhat* but it isn't a silver bullet. Light solutions like containers have very little overhead though so the benefits tend to outweigh the costs.

We'll use containers, because they're entertaining and fun to use. This is a laboratory, remember? We're suposed to have fun and play with new things.

## Our First Container

We'll be using `machinectl` to work with our containers. You may already know Docker, CoreOS's `rkt`, or `lxc`, and you're more than welcome to use any of those. The important thing is to only use **one** on this machine. You don't want to be managing multiple container providers. Good news is they're all pretty much compatible with one another, so if you use `machinectl` you can still use Docker images.

To get started with our first container, we first need to construct one. We'll make a scratch directory and put it in there. We'll tell `pacstrap`, which came as part of `arch-install-scripts`, to ignore anything that's likely unnecessary in a container.

```bash
mkdir -p ~/scratch/
echo -e "n\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\nn\n\n" | sudo pacstrap -i -c -d ~/scratch/ base --ignore linux,linux-firmware,cryptsetup,jfsutils,lvm2,nano,netctl,reiserfsprogs,vi,xfsprogs,mdadm,device-mapper,licenses,man-pages,pcmciautils
```

This should give you an installed package size of under 400MB. That's pretty decent. We could probably go smaller but it doesn't really matter. We don't need to worry about setting up things like `iptables` since it's already protected via the host, and we don't need to have things like `vi` because we'll do our editing from outside of the container. We don't need things like `linux` because we're using the host kernel.

Take a look in `~/scratch` and you should see what appears to be a base Arch install like we had before. We're going to use `chroot` to enable some services by default, finally we're going to import it into `machinectl`:

```bash
cd ~/scratch
cat <<EOF | sudo chroot .
    systemctl enable systemd-networkd
    systemctl enable systemd-resolved
    rm /etc/resolv.conf
    ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf
    sed -i -e 's/hosts: files dns myhostname/hosts: files mymachines resolve myhostname/g' /etc/nsswitch.conf
EOF
sudo tar --create --preserve-permissions --gzip * | sudo machinectl import-tar --read-only - base
```

After a second, this command will be done and we'll have our first image. `machinectl list-images` shows this:

```
NAME TYPE      RO  USAGE  CREATED                     MODIFIED
base subvolume yes 277.6M Wed 2016-05-11 15:52:43 PDT n/a     

1 images listed.
```

We can test out the image with `machinectl start base`. If you get an "Operation not supported" error here it's because you updated your kernel and haven't restarted. Later we'll talk about [`kexec`](https://wiki.archlinux.org/index.php/kexec) to help with this problem.

You can get a shell into the container with `machinectl shell base`. You'll probably notice there is no network connection. That's because our firewall is blocking us! You can check this by observing `iptables -nvL` and watching the packet counts go up on the `icmp-port-unreachable` filter.

Editing our `/etc/iptables/iptables.rules` we can add the green lines:

```diff
 # Accept anything from the local loopback.
 --append INPUT --in-interface lo --jump ACCEPT
+# Accept anything originating from a container.
+--append INPUT --in-interface ve-+ --jump ACCEPT
+--append FORWARD --in-interface ve-+ --jump ACCEPT
+--append FORWARD --out-interface ve-+ --jump ACCEPT
```

This rule will make it so any communications coming from our container adapters will be permitted accepted. Now reload the firewall with `systemctl restart iptables`.

Returning to your container with `machinectl reboot base` then `machinectl shell base`, from here you should be able to run `ping hoverbear.org` and `ping silicon` if `silicon` is your machine's hostname like mine.

You can check over your work by running `btrfs subvolume list /` and verifying that there is a `var/lib/machines/base` subvolume, then running `machinectl list-images --all` and verifying there is a `base` image.

From here you can create writable clones of the image with `machinectl clone base $NAME`, then `start` them, `shell` into them, and ultimately `poweroff` them. When you're ready to get rid of them entirely you can `remove` them.

Later on you can upgrade your `base` image in a similar way.

```bash
sudo machinectl clone base upgrade && \
sudo machinectl start upgrade && \
sleep 1 && \
sudo machinectl shell upgrade /bin/pacman -- -Syu && \
sleep 1 && \
sudo machinectl poweroff upgrade && \
sudo machinectl remove base && \
sudo machinectl read-only upgrade && \
sudo machinectl rename upgrade base
```

## Orcestration

At this point you might have identified a potential problem coming up. *How do we create and maintain all these gosh darn containers?* Well, that's a **great** question! We're going to try using [Puppet](https://docs.puppet.com/puppet/)!

This is a very different strategy compared to that taken by Docker. Docker uses declarative `Dockerfile`s to build images, then you upgrade by either replacing the current running container, or executing commands via the `run` subcommand. With `puppet` we'll be able to easily execute pre-defined provisioning scripts and maintain our running containers.

Also, I want to learn Puppet, so this is a great excuse. You can review the Puppet docs as well as [this guide by Digital Ocean](https://www.digitalocean.com/community/tutorial_series/how-to-use-puppet-to-manage-your-servers-2). Our plan is a bit different than those.

Let's start by creating a `puppet` container from our base.

```bash
sudo machinectl clone base puppet
```

Since we're basing our `puppet` image on our `base` Arch image we can have them share repository caches. Neat! Next we'll create a `environments` subvolume on our BTRFS volume so we can mount it into the `puppet` image. This will help us not accidently delete things.

```bash
mount /dev/sda /mnt/
cd /mnt/
btrfs subvolume create puppet
echo "/dev/sda    /puppet             btrfs   subvol=puppet,rw,relatime,ssd,space_cache,compress=lzo          0   0" >> /etc/fstab
mount /puppet
mkdir -p /puppet
```


We'll create `/etc/systemd/nspawn/puppet.nspawn` which is the container specific configuration.

```bash
mkdir -p /etc/systemd/nspawn/
cat <<EOF > /etc/systemd/nspawn/puppet.nspawn
[Files]
# Official packages are signed and verified before install
Bind=/var/cache/pacman
# abs packages are not.
BindReadOnly=/var/cache/abs
# Bind in our puppet configuration.
Bind=/puppet:/etc/puppetlabs
EOF
```

Now we can start the machine and hop inside.

```bash
sudo machinectl start puppet
sudo machinectl shell puppet
```

Inside of this container we can start seting up Puppet! First we'll install it:

```bash
pacman -Syu puppet --noconfirm
```

Before we start making manifests we need to get the server working though! In order to do this we need to edit our configuration file at `/etc/puppetlabs/puppet/puppet.conf`.

```bash
cat <<EOF >> /etc/puppetlabs/puppet/puppet.conf
[main]
certname = puppet
environment = production
runinterval = 1h
strict_variables = true
EOF
```

Next we'll make a simple `.service` file for `puppet master` since it doesn't come with one by default. You can run `systemctl cat puppet` to see the configuration of `puppet agent`. We'll basically just copy this.

```bash
cat <<EOF > /etc/systemd/system/puppetmaster.service
[Unit]
Description=Puppet master
Wants=basic.target
After=basic.target network.target

[Service]
EnvironmentFile=-/etc/sysconfig/puppetmaster
EnvironmentFile=-/etc/sysconfig/puppet
EnvironmentFile=-/etc/default/puppet
ExecStart=/usr/bin/puppet master $PUPPET_EXTRA_OPTS --no-daemonize
ExecReload=/bin/kill -HUP $MAINPID
KillMode=process

[Install]
WantedBy=multi-user.target
EOF
```

Now you can start it with `systemctl start puppetmaster`. At this point let's consider that our `puppet` container should probably be managed by Puppet as well! Let's do that to learn how to use Puppet, then we'll figure out networking.

```bash
mkdir -p /etc/puppetlabs/code/environments/production/manifests/
cat <<EOF > /etc/puppetlabs/code/environments/production/manifests/main.pp
node puppet,test {
    file { 'puppet-test':
        path    => '/puppet-test',
        ensure  => file,
        content => "Hello. Hallo.",
        owner   => 'root',
        mode    => '0644',
    }
}
EOF
```

Now you can run `systemctl start puppet` (this is the agent) and you can see that `/puppet-test` now exists. Finally run `systemctl enable puppet` and `systemctl enable puppetmaster`. Remember that you can edit this configuration from outside the container by editing the things in `/puppet` on the our root.

All we need to do now is add Puppet to our `base`. First make it writable with `machinectl read-only base false`, start it, then shell in with `machinectl shell base`.

```bash
pacman -Syu puppet --noconfirm
systemctl enable puppet
```

Then `poweroff` it and mark it read only again with `machinectl read-only base true`. Now when we clone it the new containers will automatically run puppet.

## Orcestration Network

Right now if you start up `base` and try to `ping puppet` from it things it won't work. You can observe the differences in their IP addresses. Mine were `169.254.143.177/16` and `169.254.221.43/16` which are on different subnets. We need some what to bring them together.


Next, since we'll be adding this link to **all** containers except `puppet` itself, we'll run `systemctl edit systemd-nspawn@` and add this:

```
[Service]
# Create adapter.
ExecStartPre=/usr/bin/ip link add pup-%i type veth peer name cl-%i
# Add endpoint to puppet.
ExecStartPre=/usr/bin/sh -c "machinectl show --property=Leader puppet | cut -d \"=\" -f 2 | /usr/bin/xargs /usr/bin/ip link set cl-%i netns"
# Create container, sink one end.
ExecStart=
ExecStart=/usr/bin/systemd-nspawn --quiet --keep-unit --boot --link-journal=try-guest --network-veth --network-interface=pup-%i --settings=override --machine=%I
# Tear down.
ExecStopPost=-/usr/bin/ip link delete cl-%i

[Unit]
Requires=systemd-nspawn@puppet.service
After=systemd-nspawn@puppet.service
```

Then run `systemctl edit systemd-nspawn@puppet` to exclude this setting. The most specific setting will win, so this overrides our override:

```
[Service]
ExecStart=
ExecStart=/usr/bin/systemd-nspawn --quiet --keep-unit --boot --link-journal=try-guest --network-veth --settings=override --machine=%I
```

At this point we just need to reconfigure our `base` image to handle any `pup-*` interfaces, then configure the `puppet` image to handle the `cl-*` interfaces.

```bash
machinectl read-only base false
cat <<EOF > /var/lib/machines/base/etc/systemd/network/pup.network
[Match]
Name=pup-*

[Network]
Description=Puppet agent connection to the master.
DHCP=ipv4
EOF
machinectl read-only base true
cat <<EOF > /var/lib/machines/puppet/etc/systemd/network/src.network
[Match]
Name=cl-*

[Network]
Description=Puppet master connection to the agent.
DHCP=ipv4
EOF
```

Try issuing `machinectl clone base test` `machinectl poweroff test puppet`, `machinectl start test puppet`, then try running `machinectl shell test` and see if you can `ping puppet`.

Finally, let's make sure we can't play with other container connections. `machinectl clone base test2`, `machinectl start test2`, `machinectl shell test2`, then see if you can `ping test`. You can't! Perfect!

## The Upgrade Problem

Earlier you made have already run into this problem, but if we run `pacman -Syu` and the `linux` package on the 'root' container gets upgraded this can cause some problems for us in the long haul. This may manifest as "Operation Not Permitted" errors when using tools like `ip`, this is because the kernel is trying to access modules on the file system which no longer exist.

We can use a tool called `kexec` to quickly (and uncleanly) switch to the new kernel. On my server this takes about a third of the time compared to actually rebooting, and can involve a web console reset. It's important to note that this doesn't *replace* a reboot though, as it doesn't make efforts to clean up anything. Things may go funky, and you may still need to reboot.

We'll build a systemd unit for this then enable it like so:

```bash
pacman -S kexec-tools
cat <<EOF > /etc/systemd/system/kexec-load@.service
[Unit]
Description=Load %i as the kernel.
Documentation=man:kexec(8)
DefaultDependencies=no
Before=shutdown.target umount.target final.target

[Service]
Type=oneshot
ExecStart=/usr/bin/kexec -l /boot/vmlinuz-%i --initrd=/boot/initramfs-%i.img --reuse-cmdline

[Install]
WantedBy=kexec.target
EOF
systemctl enable kexec-load@linux
```

Now you can load into the newest kernel with `systemctl kexec` when you decide it's time. I'd suggest testing it now before moving on so you know it works!

## The Web Proxy

Since we know we'd like to host be able to host multiple web services we'll need to set up a *proxy* to handle port 80, the default socket for `httpd`. Since only one program can bind to a given port at a time, its job will be to inspect the incoming traffic to observe the *Server Name Indentification*, or *SNI*, then route it to the appropriate container. By default this will be the `web-$VARIABLE` container, where the request is `$VARIABLE.$DOMAIN.$TLD`. We'll call it `web`.

```
[network: example.hoverbear.org]->[web]->[web-example]
```

Later we'll follow a similar scheme for `db`, `user`, `sandbox`, `cache`, etc. You can use whichever scheme you'd like, or change later, just **be consistent**.

We'll start by editing our puppet configuration to define what this host should look like. Since we have the puppet configuration subvolume mounted on `/puppet` on our host we can just edit it from there.

```bash
mkdir -p /puppet/code/environments/production/manifests/files/web/
cat <<EOF > /puppet/code/environments/production/manifests/files/web/nginx.conf
worker_processes  auto;
worker_rlimit_nofile 8192;

events {
  worker_connections  4096;
}
http {
    server {
        listen 443 ssl spdy;
        proxy_pass https://web-$server_name$request_uri;

    }
    # Redirect everything to HTTPS. All of our hosts should be encrypted.
    server {
        listen 80;
        return 301 https://$server_name$request_uri;
    }
}
EOF
cat <<EOF > /puppet/code/environments/production/manifests/main.pp
node web {
    file { 'nginx.conf':
        path    => '/etc/nginx/nginx.conf',
        ensure  => file,
        content => template('web/nginx/nginx.conf'),
        owner   => 'root',
        mode    => '0644',
    }
    package web {
        package {'nginx':
        ensure => installed,
        before => File['nginx.conf'],
    }
    service { 'nginx':
        ensure    => running,
        subscribe => File['nginx.conf'],
    }
}
EOF
```
